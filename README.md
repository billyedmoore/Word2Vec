# Experimentation with Google's Word2Vec

Implementation and experimentation with famous text embedding model.

## Useful Resources

Useful resources to understand Word2Vec.

+ [The original paper](https://arxiv.org/pdf/1301.3781)
+ [Blog post about building Word2Vec from scratch in Numpy by Jake Tae](https://jaketae.github.io/study/word2vec/)
+ [Blog post explaining the Word2Vec model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) 
  and [follow up explaining subsampling negative sampling](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/) by Chris McCormick

## Environment

The environment variable `KERAS_BACKEND=torch` must be set (unless torch is configured as the default Keras backend in your config).

